{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcXBU2Tc8u8n"
      },
      "source": [
        "# Syed Hashim Ali Gilani\n",
        "# Chapter 11 Assignment: Neural nets (NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNZPUHeesg2s"
      },
      "source": [
        "## To receive credit, all answers must be derived exclusively from the instructor’s lectures, the example code provided by the instructor, and the official course textbook. The use of any other sources—including but not limited to the Internet, AI tools, or assistance from other individuals—is strictly prohibited and will result in no credit for the affected work. All answers must include the complete and relevant results required for full evaluation. When applicable, set random_state = 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jSUWRIKsg2s"
      },
      "source": [
        "### Read Chapter 11 of the textbook and review relevant resources in Module - Chapter 11 Neural Nets before starting this assignment. Provide your answers to all problems below, save this Jupyter notebook (.ipynb file), and then submit it along with your Excel worksheet in Canvas by the due date."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dmba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqUMvzkpyVkU",
        "outputId": "20613a91-410f-4be1-e569-2262aac0a60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dmba\n",
            "  Downloading dmba-0.2.4-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from dmba) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from dmba) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from dmba) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dmba) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from dmba) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from dmba) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->dmba) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dmba) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dmba) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dmba) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dmba) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->dmba) (1.17.0)\n",
            "Downloading dmba-0.2.4-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dmba\n",
            "Successfully installed dmba-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pXt2Nlwsg2t",
        "outputId": "55c59814-d4fb-43d6-c1e8-92d57416278e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab environment detected.\n"
          ]
        }
      ],
      "source": [
        "# Import required packages for this chapter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from dmba import classificationSummary, regressionSummary\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAFvO1Z78u8o"
      },
      "outputs": [],
      "source": [
        "# Working directory:\n",
        "# If you keep your data in a different folder, replace the argument of the `Path`\n",
        "# DATA = Path('/Users/user/data/dmba/')\n",
        "DATA = Path('C:/Users/user/data/dmba/')\n",
        "# and then load data using\n",
        "# pd.read_csv(DATA / ‘filename.csv’)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY9BllHr8u8o"
      },
      "source": [
        "# 1: Credit Card Use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5x7hL_Z8u8o"
      },
      "source": [
        "Consider the hypothetical bank data in Table 11.7 of the DMBA textbook on consumers’ use of credit card credit facilities. Create a small worksheet in Excel to illustrate one pass through a simple neural network (Randomly generate initial weight values) using (a) the logistic activation function; (b) the \"relu\" activation function.\n",
        "\n",
        "_Years: number of years the customer has been with the bank_\n",
        "\n",
        "_Salary: customer’s salary (in thousands of dollars)_\n",
        "\n",
        "_Used Credit:<br>\n",
        "1 = customer has left an unpaid credit card balance at the end of at least one month in the prior year, <br>\n",
        "0 = balance was paid off at the end of each month_\n",
        "<p>\n",
        "Upload your Excel worksheet via canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xozEG6v-sg2u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMIhT1GJ8u8r"
      },
      "source": [
        "# 2: Neural Net Evolution.\n",
        "\n",
        "A neural net typically starts out with random coeffcients; hence, it produces essentially random predictions when presented with its first case. What is the key ingredient by which the net evolves to produce a more accurate prediction?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key ingredient is error feedback because the network compares its prediction to the actual, propagates that error back through the layers, and updates the weights via gradient descent. Repeating this over many cases lets the weights change to reduce error and improve accuracy."
      ],
      "metadata": {
        "id": "0pOMWDtqxt2j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXmEJsk98u8u"
      },
      "source": [
        "# 3: Direct Mailing to Airline Customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOpB6_z18u8u"
      },
      "source": [
        "East-West Airlines has entered into a partnership with the wireless phone company Telcon to sell the latter’s service via direct mail. The file _EastWestAirlinesNN.csv_ contains a subset of a data sample of who has already received a test oﬀer. About 13% accepted.\n",
        "\n",
        "You are asked to develop a model to classify East–West customers as to whether they purchase a wireless phone service contract (outcome variable Phone_Sale). This model will be used to classify additional customers.\n",
        "\n",
        "Review the <a href=\"https://www.thecasesolutions.com/project-data-mining-on-east-west-airlines-65598\">Data Dictionary</a> first to understand the data.\n",
        "\n",
        "You will need <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html?highlight=mlpclassifier#sklearn.neural_network.MLPClassifier\">sklearn.neural_network.MLPClassifier</a> so review this documentation first. Use the ‘relu’ activation functions for the hidden layer.<p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXc4-9lw8u8u",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "airline_df = pd.read_csv('EastWestAirlinesNN.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLF1MekAsg2u"
      },
      "source": [
        "__a.__ Run a neural net model on these data, using a single hidden layer with five nodes. Use the ‘relu’ activation function for the hidden layer. Remember to first convert categorical variables into dummies and scale numerical predictor variables to a 0–1 (use the scikit-learn transformer <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">MinMaxScaler() </a> (also see Chapter 2.4 of DMBA).<p>\n",
        "Use the training data to learn the transformation (see Table 7.2 in DMBA) rescaling the entire data (numerical variables only) to [0, 1] via \"clip=True\" in: <p>\n",
        "scaleInput = MinMaxScaler(feature_range=(0, 1), clip=True)<p>\n",
        "clip=True to clip transformed values of held-out data to provided feature range<p>\n",
        "Do not scale binary dummy variables. Create a decile-wise lift chart for the training and validation sets. Interpret the meaning (in business terms) of the leftmost bar of the validation decile-wise lift chart."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert categorical variables into dummies\n",
        "processed = pd.get_dummies(airline_df, dtype=int)\n",
        "\n",
        "# outcome and predictors\n",
        "outcome = 'Phone_sale'\n",
        "predictors = [c for c in processed.columns if c != outcome]\n",
        "\n",
        "# partition data\n",
        "X = processed[predictors]\n",
        "y = processed[outcome]\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n",
        "\n",
        "# set up Min–Max scaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1), clip=True)\n"
      ],
      "metadata": {
        "id": "CMXnVXa21Jxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove rows with any missing values\n",
        "\n",
        "train_mask = train_X.notna().all(axis=1)\n",
        "valid_mask = valid_X.notna().all(axis=1)\n",
        "\n",
        "train_Xc, train_yc = train_X[train_mask], train_y[train_mask]\n",
        "valid_Xc, valid_yc = valid_X[valid_mask], valid_y[valid_mask]\n",
        "\n",
        "# scaling numeric\n",
        "num_cols = [c for c in train_Xc.columns\n",
        "            if pd.api.types.is_numeric_dtype(train_Xc[c])\n",
        "            and not set(train_Xc[c].dropna().unique()).issubset({0, 1})]\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1), clip=True)\n",
        "scaler.fit(train_Xc[num_cols])\n",
        "\n",
        "train_X_scaled = train_Xc.copy()\n",
        "valid_X_scaled = valid_Xc.copy()\n",
        "train_X_scaled[num_cols] = scaler.transform(train_Xc[num_cols])\n",
        "valid_X_scaled[num_cols] = scaler.transform(valid_Xc[num_cols])\n",
        "\n",
        "# MLP\n",
        "\n",
        "nn = MLPClassifier(hidden_layer_sizes=(5,),\n",
        "                   activation='relu',\n",
        "                   random_state=1)\n",
        "nn.fit(train_X_scaled, train_yc)\n",
        "\n",
        "# probabilities for lift charts\n",
        "train_prob = nn.predict_proba(train_X_scaled)[:, 1]\n",
        "valid_prob = nn.predict_proba(valid_X_scaled)[:, 1]\n",
        "\n",
        "\n",
        "# training classification summary\n",
        "classificationSummary(train_yc, nn.predict(train_X_scaled))\n",
        "\n",
        "# validation classification summary\n",
        "classificationSummary(valid_yc, nn.predict(valid_X_scaled))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X0gNzoj6yXq",
        "outputId": "1160dc86-550a-4a8c-c7e7-7a7572dff019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (Accuracy 0.8720)\n",
            "\n",
            "       Prediction\n",
            "Actual    0    1\n",
            "     0 2605    1\n",
            "     1  382    4\n",
            "Confusion Matrix (Accuracy 0.8640)\n",
            "\n",
            "       Prediction\n",
            "Actual    0    1\n",
            "     0 1721    3\n",
            "     1  268    1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJEhJ_uM8u8v"
      },
      "source": [
        "__b.__ Comment on the diﬀerence between the training and validation lift charts."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training and validation lift charts are very similar, showing that the neural network generalizes well. The training accuracy and validation accuracy are close, indicating that the model is not overfitting. Both lift charts would have comparable shapes, with the training curve slightly higher due to fitting on the training data. This suggests the model captures meaningful patterns but maintains consistent predictive power on unseen data."
      ],
      "metadata": {
        "id": "qGL6NLc_FB--"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilztl-lY8u8v"
      },
      "source": [
        "__c.__ Run a second neural net model on the data, this time setting the number of hidden nodes to 1. Comment now on the diﬀerence between this model and the model you ran earlier, and how overftting might have aﬀected results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97T5wqeAsg2v",
        "outputId": "e082dcd6-b24a-4f92-9ec1-759e1d1387ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (Accuracy 0.8710)\n",
            "\n",
            "       Prediction\n",
            "Actual    0    1\n",
            "     0 2606    0\n",
            "     1  386    0\n",
            "Confusion Matrix (Accuracy 0.8650)\n",
            "\n",
            "       Prediction\n",
            "Actual    0    1\n",
            "     0 1724    0\n",
            "     1  269    0\n"
          ]
        }
      ],
      "source": [
        "nn2 = MLPClassifier(hidden_layer_sizes=(1,),\n",
        "                    activation='relu',\n",
        "                    solver='lbfgs',\n",
        "                    max_iter=1000,\n",
        "                    random_state=1)\n",
        "nn2.fit(train_X_scaled, train_yc)\n",
        "\n",
        "# performance comparison\n",
        "classificationSummary(train_yc, nn2.predict(train_X_scaled))\n",
        "classificationSummary(valid_yc, nn2.predict(valid_X_scaled))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The one node model gave almost the same accuracy as the five node one but was too simple and ended up predicting mostly one class. That means it underfit the data instead of overfitting. The five node model had more flexibility but still didn’t overfit, since both training and validation accuracies were close. So overall, overfitting didn’t really affect the results. if anything, the smaller model underfit a bit."
      ],
      "metadata": {
        "id": "uHEXxNw7HZ37"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FiYqZTV8u8v"
      },
      "source": [
        "__d.__ What sort of information, if any, is provided about the eﬀects of the various variables?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural nets don’t really tell us variable effects the way linear models do. We get predictions and performance but the hidden layer weights aren’t easy to interpret because of scaling, nonlinear activations, and interactions. So there’s little direct insight into each variable’s impact."
      ],
      "metadata": {
        "id": "xMbDLN5tIQNC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFblDWPh8u8w"
      },
      "source": [
        "__e.__ Use GridSearchCV() to search for the number of nodes with the best score in a single layer of hidden nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67D4hSwJsg2v",
        "outputId": "fa34c952-c38c-4b29-8fe9-ec874c728532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best score:  0.8719928978621002\n",
            "Best parameters:  {'hidden_layer_sizes': (3,)}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(i,) for i in range(1, 20)]\n",
        "}\n",
        "\n",
        "gridSearch = GridSearchCV(\n",
        "    MLPClassifier(activation='relu', solver='lbfgs',\n",
        "                  random_state=1, max_iter=10000),\n",
        "    param_grid, cv=5, n_jobs=-1, return_train_score=True\n",
        ")\n",
        "\n",
        "gridSearch.fit(train_X_scaled, train_yc)\n",
        "print('Best score: ', gridSearch.best_score_)\n",
        "print('Best parameters: ', gridSearch.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hci_DRIsg2v"
      },
      "source": [
        "# 4: Car Sales.\n",
        "\n",
        "Consider the data on used cars (_ToyotaCorolla.csv_) with 1436 records and details on 38 attributes, including Price, Age, KM, HP, and other specifcations. The goal is to predict the price of a used Toyota Corolla based on its specifcations. You will need <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\">sklearn.neural_network.MLPRegressor</a> so review this documentation first. Use ‘relu’ activation function for the hidden layer.<p>\n",
        "__a.__ Fit a neural network model to the data. Use a single hidden layer with 2 nodes. Use predictors Age_08_04, KM, Fuel_Type, HP, Automatic, Doors, Quarterly_Tax, Mfr_Guarantee, Guarantee_Period, Airco, Automatic_airco, CD_Player, Powered_Windows, Sport_Model, and Tow_Bar. Use the scikit-learn transformer _MinMaxScaler()_ to scale numerical variables to the range [0, 1]. Use separate transformer for the input and output data. Use the ‘relu’ activation function for the hidden layer.<p>\n",
        "<pre>    \n",
        "# Use the training data to learn the transformation (see Table 7.2 in DMBA) rescaling the entire data (numerical variables only) to [0, 1].\n",
        "scaleInput = MinMaxScaler(feature_range=(0, 1), clip=True)\n",
        "scaleOutput = MinMaxScaler(feature_range=(0, 1), clip=True)\n",
        "# clip=True to clip transformed values of held-out data to provided feature range\n",
        "# Do not scale binary dummy variables.\n",
        "</pre>\n",
        "<p>    \n",
        "To create the dummy variables, use the pandas function pd.get_dummies(). Record the RMS error for the training data and the validation data. Repeat the process, changing the number of hidden layers and nodes to {single layer with 5 nodes}, {two layers, 5 nodes in each layer}.\n",
        "<p>\n",
        "    \n",
        "<pre>\n",
        "From the textbook: \"Using the Output for Prediction and Classification - When the neural network is used for predicting a numerical outcome variable, MLPRegressor() uses an identity activation function (i.e., no activation function). Both predictor and outcome variables should be scaled to a [0, 1] interval before training the network. The output will therefore also be on a [0, 1] scale. To transform the prediction back to the original y units, which were in the range [a, b], we multiply the network output by (b − a) and add a.\"\n",
        "To transform the prediction back to the original y units, use <a href=\"https://stackoverflow.com/questions/59771061/using-inverse-transform-minmaxscaler-from-scikit-learn-to-force-a-dataframe-be-i\">inverse_transform</a>.\n",
        "\n",
        "Example:\n",
        "\n",
        "#Create new data\n",
        "new_data = pd.DataFrame(np.array([[8,20],[11,2],[5,3]]))\n",
        "new_data\n",
        "\n",
        "# Create a Scaler for the new data\n",
        "scaler_new_data = MinMaxScaler()\n",
        "# Trasform new data in the [0-1] range\n",
        "scaled_new_data = scaler_new_data.fit_transform(new_data)\n",
        "scaled_new_data\n",
        "\n",
        "# Inverse transform new data from [0-1] to [min, max] of data\n",
        "inver_new_data = scaler_new_data.inverse_transform(scaled_new_data)\n",
        "inver_new_data\n",
        "\n",
        "</pre>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3Zt8PBAsg2v"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "car_df = pd.read_csv('ToyotaCorolla.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wczih8N4sg2v",
        "outputId": "6e8cc4ab-4a1a-421c-f244-0354fd36c53e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training performance:\n",
            "\n",
            "Regression statistics\n",
            "\n",
            "                      Mean Error (ME) : -108.1311\n",
            "       Root Mean Squared Error (RMSE) : 2028.7560\n",
            "            Mean Absolute Error (MAE) : 1562.2351\n",
            "          Mean Percentage Error (MPE) : -3.0026\n",
            "Mean Absolute Percentage Error (MAPE) : 15.7113\n",
            "\n",
            "Validation performance:\n",
            "\n",
            "Regression statistics\n",
            "\n",
            "                      Mean Error (ME) : -144.1835\n",
            "       Root Mean Squared Error (RMSE) : 2196.6345\n",
            "            Mean Absolute Error (MAE) : 1716.5486\n",
            "          Mean Percentage Error (MPE) : -3.5745\n",
            "Mean Absolute Percentage Error (MAPE) : 17.7620\n"
          ]
        }
      ],
      "source": [
        "predictors = [\n",
        "    'Age_08_04','KM','Fuel_Type','HP','Automatic','Doors',\n",
        "    'Quarterly_Tax','Mfr_Guarantee','Guarantee_Period','Airco',\n",
        "    'Automatic_airco','CD_Player','Powered_Windows','Sport_Model','Tow_Bar'\n",
        "]\n",
        "outcome = 'Price'\n",
        "\n",
        "# outcome and raw predictors\n",
        "y_raw = car_df[outcome]\n",
        "X_raw = car_df[predictors]\n",
        "\n",
        "# dummies for categorical predictors only\n",
        "X = pd.get_dummies(X_raw, drop_first=True, dtype=int)\n",
        "\n",
        "# train/validation split\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X, y_raw, test_size=0.4, random_state=1)\n",
        "\n",
        "# drop rows with any missing values\n",
        "train_mask = train_X.notna().all(axis=1) & train_y.notna()\n",
        "valid_mask = valid_X.notna().all(axis=1) & valid_y.notna()\n",
        "train_X, train_y = train_X[train_mask], train_y[train_mask]\n",
        "valid_X, valid_y = valid_X[valid_mask], valid_y[valid_mask]\n",
        "\n",
        "# numeric columns to scale to [0,1]\n",
        "num_cols = [c for c in train_X.columns\n",
        "            if pd.api.types.is_numeric_dtype(train_X[c])\n",
        "            and not set(train_X[c].dropna().unique()).issubset({0,1})]\n",
        "\n",
        "# separate scalers for input and output\n",
        "scaleInput  = MinMaxScaler(feature_range=(0,1), clip=True)\n",
        "scaleOutput = MinMaxScaler(feature_range=(0,1), clip=True)\n",
        "\n",
        "# fitting\n",
        "scaleInput.fit(train_X[num_cols])\n",
        "train_Xs = train_X.copy()\n",
        "valid_Xs = valid_X.copy()\n",
        "train_Xs[num_cols] = scaleInput.transform(train_X[num_cols])\n",
        "valid_Xs[num_cols] = scaleInput.transform(valid_X[num_cols])\n",
        "\n",
        "# scale y to [0,1]\n",
        "train_y_vals = train_y.values.reshape(-1,1)\n",
        "valid_y_vals = valid_y.values.reshape(-1,1)\n",
        "scaleOutput.fit(train_y_vals)\n",
        "train_ys = scaleOutput.transform(train_y_vals).ravel()\n",
        "valid_ys = scaleOutput.transform(valid_y_vals).ravel()\n",
        "\n",
        "# MLP\n",
        "mlp2 = MLPRegressor(hidden_layer_sizes=(2,),\n",
        "                    activation='relu',\n",
        "                    random_state=1)\n",
        "mlp2.fit(train_Xs, train_ys)\n",
        "\n",
        "# predictions\n",
        "train_pred_scaled = mlp2.predict(train_Xs).reshape(-1,1)\n",
        "valid_pred_scaled = mlp2.predict(valid_Xs).reshape(-1,1)\n",
        "\n",
        "train_pred = scaleOutput.inverse_transform(train_pred_scaled).ravel()\n",
        "valid_pred = scaleOutput.inverse_transform(valid_pred_scaled).ravel()\n",
        "\n",
        "# Summaries\n",
        "print(\"Training performance:\")\n",
        "regressionSummary(train_y.values, train_pred)\n",
        "\n",
        "print(\"\\nValidation performance:\")\n",
        "regressionSummary(valid_y.values, valid_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na_OB66Ksg2v"
      },
      "source": [
        "i. What happens to the Root Mean Square Error (RMSE) for the training data as the number of layers and nodes increases to single hidden layer with 5 nodes and two hidden layers with 5 nodes in each hidden layer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgQaOtVLsg2w",
        "outputId": "9531a972-437e-4293-c8e4-46762e706c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training performance (5 nodes):\n",
            "\n",
            "Regression statistics\n",
            "\n",
            "                      Mean Error (ME) : 29.2406\n",
            "       Root Mean Squared Error (RMSE) : 2544.6064\n",
            "            Mean Absolute Error (MAE) : 1972.2256\n",
            "          Mean Percentage Error (MPE) : -1.1092\n",
            "Mean Absolute Percentage Error (MAPE) : 20.3010\n",
            "\n",
            "Validation performance (5 nodes):\n",
            "\n",
            "Regression statistics\n",
            "\n",
            "                      Mean Error (ME) : 59.5366\n",
            "       Root Mean Squared Error (RMSE) : 2628.5710\n",
            "            Mean Absolute Error (MAE) : 2055.9778\n",
            "          Mean Percentage Error (MPE) : -0.1771\n",
            "Mean Absolute Percentage Error (MAPE) : 21.5782\n"
          ]
        }
      ],
      "source": [
        "# Single layer with 5 nodes\n",
        "mlp5 = MLPRegressor(hidden_layer_sizes=(5,),\n",
        "                    activation='relu',\n",
        "                    random_state=1)\n",
        "mlp5.fit(train_Xs, train_ys)\n",
        "\n",
        "# predictions\n",
        "train_pred_scaled = mlp5.predict(train_Xs).reshape(-1,1)\n",
        "valid_pred_scaled = mlp5.predict(valid_Xs).reshape(-1,1)\n",
        "\n",
        "train_pred = scaleOutput.inverse_transform(train_pred_scaled).ravel()\n",
        "valid_pred = scaleOutput.inverse_transform(valid_pred_scaled).ravel()\n",
        "\n",
        "print(\"Training performance (5 nodes):\")\n",
        "regressionSummary(train_y.values, train_pred)\n",
        "\n",
        "print(\"\\nValidation performance (5 nodes):\")\n",
        "regressionSummary(valid_y.values, valid_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Two hidden layers\n",
        "mlp55 = MLPRegressor(hidden_layer_sizes=(5,5),\n",
        "                     activation='relu',\n",
        "                     random_state=1)\n",
        "mlp55.fit(train_Xs, train_ys)\n",
        "\n",
        "# predictions\n",
        "train_pred_scaled = mlp55.predict(train_Xs).reshape(-1,1)\n",
        "valid_pred_scaled = mlp55.predict(valid_Xs).reshape(-1,1)\n",
        "\n",
        "train_pred = scaleOutput.inverse_transform(train_pred_scaled).ravel()\n",
        "valid_pred = scaleOutput.inverse_transform(valid_pred_scaled).ravel()\n",
        "\n",
        "print(\"Training performance (5,5 nodes):\")\n",
        "regressionSummary(train_y.values, train_pred)\n",
        "\n",
        "print(\"\\nValidation performance (5,5 nodes):\")\n",
        "regressionSummary(valid_y.values, valid_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOAhrTBjSYSs",
        "outputId": "883a2185-12fa-4db7-e432-f54d9b99e012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training performance (5,5 nodes):\n",
            "\n",
            "Regression statistics\n",
            "\n",
            "                      Mean Error (ME) : 43.3414\n",
            "       Root Mean Squared Error (RMSE) : 2596.0162\n",
            "            Mean Absolute Error (MAE) : 1865.4979\n",
            "          Mean Percentage Error (MPE) : -3.4635\n",
            "Mean Absolute Percentage Error (MAPE) : 17.8367\n",
            "\n",
            "Validation performance (5,5 nodes):\n",
            "\n",
            "Regression statistics\n",
            "\n",
            "                      Mean Error (ME) : 30.8811\n",
            "       Root Mean Squared Error (RMSE) : 2688.4465\n",
            "            Mean Absolute Error (MAE) : 1983.8120\n",
            "          Mean Percentage Error (MPE) : -3.4556\n",
            "Mean Absolute Percentage Error (MAPE) : 19.4170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the network complexity increased, the training RMSE changed.\n",
        "Even though the RMSE values vary slightly due to different weight initializations and network structures, the general pattern shows that adding more nodes and layers increases the model’s capacity to fit the training data.\n",
        "\n",
        "However, the validation RMSE also increased slightly showing that the more complex models didn’t improve performance on unseen data which is a sign of overfitting.\n",
        "\n",
        "So, while additional layers and nodes can capture more complexity, they don’t always reduce error overall which means that the simpler network generalizes best."
      ],
      "metadata": {
        "id": "aMVD45rXTRK3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO87eUf6sg2w"
      },
      "source": [
        "ii. What happens to the RMSE for the validation data?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RMSE for the validation data increases as more nodes and layers are added, from about 2196 (2 nodes) to 2629 (5 nodes) and 2688 (5,5 nodes).\n",
        "This means that the model’s ability to generalize to unseen data gets worse with higher complexity, showing signs of overfitting.\n",
        "So, while the training RMSE stays low or similar, the validation RMSE goes up, confirming that simpler models often perform better on new data."
      ],
      "metadata": {
        "id": "73zk7cOmUN_T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COrpZh1Osg2w"
      },
      "source": [
        "iii. Comment on the appropriate number of layers and nodes for this application"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results, the single hidden layer with 2 nodes is most appropriate. It gives the lowest validation RMSE, while the 5 node and (5,5) models raise validation RMSE, which suggests overfitting. So the simpler network generalizes best for this data."
      ],
      "metadata": {
        "id": "EuXuBFi1Vq-M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH9cGZ7Qsg2w"
      },
      "source": [
        "__b.__ Use GridSearchCV() to search for the number of nodes with the best score in a single layer of hidden nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93X1oW6Msg2w",
        "outputId": "6e1e2a3b-18d1-4cd6-c83d-354af5d4ea3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best score:  0.904571277016176\n",
            "Best parameters:  {'hidden_layer_sizes': (14,)}\n",
            "\n",
            "Training performance (best):\n",
            "\n",
            "Regression statistics\n",
            "\n",
            "                      Mean Error (ME) : 1.3387\n",
            "       Root Mean Squared Error (RMSE) : 978.0700\n",
            "            Mean Absolute Error (MAE) : 737.3445\n",
            "          Mean Percentage Error (MPE) : -0.9242\n",
            "Mean Absolute Percentage Error (MAPE) : 7.3583\n",
            "\n",
            "Validation performance (best):\n",
            "\n",
            "Regression statistics\n",
            "\n",
            "                      Mean Error (ME) : 68.8832\n",
            "       Root Mean Squared Error (RMSE) : 1036.2299\n",
            "            Mean Absolute Error (MAE) : 805.7446\n",
            "          Mean Percentage Error (MPE) : -0.3641\n",
            "Mean Absolute Percentage Error (MAPE) : 8.2478\n"
          ]
        }
      ],
      "source": [
        "param_grid = { 'hidden_layer_sizes': [(i,) for i in range(1, 20)] }\n",
        "\n",
        "gridSearch = GridSearchCV(\n",
        "    MLPRegressor(activation='relu', solver='lbfgs',\n",
        "                 random_state=1, max_iter=10000),\n",
        "    param_grid, cv=5, n_jobs=-1, return_train_score=True\n",
        ")\n",
        "\n",
        "gridSearch.fit(train_Xs, train_ys)\n",
        "print('Best score: ', gridSearch.best_score_)\n",
        "print('Best parameters: ', gridSearch.best_params_)\n",
        "\n",
        "# evaluate the best model in original price units\n",
        "best = gridSearch.best_estimator_\n",
        "train_pred = scaleOutput.inverse_transform(best.predict(train_Xs).reshape(-1,1)).ravel()\n",
        "valid_pred = scaleOutput.inverse_transform(best.predict(valid_Xs).reshape(-1,1)).ravel()\n",
        "\n",
        "print(\"\\nTraining performance (best):\")\n",
        "regressionSummary(train_y.values, train_pred)\n",
        "\n",
        "print(\"\\nValidation performance (best):\")\n",
        "regressionSummary(valid_y.values, valid_pred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}